{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5279b153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch import nn, optim\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7202df7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    padded_batch = []\n",
    "    max_length = max(sample[0].shape[1] for sample in batch)\n",
    "#     print('max_length', max_length)\n",
    "    for sample in batch:\n",
    "        pad_length = max_length - sample[0].shape[1]\n",
    "        padded_sample = torch.nn.functional.pad(sample[0], (0, pad_length))\n",
    "        batch_label = sample[1]\n",
    "        padded_batch.append((padded_sample,int(batch_label)))\n",
    "        \n",
    "#     print('padded_batch',padded_batch)\n",
    "    return padded_batch\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c16e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "        self.file_list = os.listdir(folder_path)\n",
    "        self.sample = []\n",
    "        for i in range(len(self.file_list)):\n",
    "            file_name = self.file_list[i]\n",
    "            file_path = os.path.join(self.folder_path, file_name)\n",
    "            data = torch.load(file_path)  ## nested list of [ file name, tensors ]\n",
    "            label = self.file_list[i].split('.')[0]\n",
    "            for tensor in data:\n",
    "                self.sample.append((tensor[1], label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample)   ## dont hard code\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print('self.sample[idx]',self.sample[idx])\n",
    "        return self.sample[idx]  # returns only samples ???? labels????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c2bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"mad_trail/train/\"\n",
    "dev_folder = \"mad_trail/dev/\"\n",
    "save_path = 'model.pt'\n",
    "train_dataset = CustomDataset(train_folder)\n",
    "dev_dataset = CustomDataset(dev_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9580d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce28b7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loader = next(iter(train_dataloader))\n",
    "\n",
    "for i in train_loader:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c80ad910",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lstm blocks   ******** IMPROVE  *********\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        ### 1D CNN\n",
    "\n",
    "        self.layer1 = torch.nn.Conv1d(in_channels=input_size, out_channels= hidden_size, kernel_size=5, stride=2)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "\n",
    "        ## BiLSTM\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.lstm_drop = nn.Dropout(0.1)\n",
    "\n",
    "        # linear \n",
    "        self.linear = nn.Linear(hidden_size*2, output_size)  ### BiLSTM\n",
    "        self.linear_drop = nn.Dropout(0)\n",
    "        ## softmax layer\n",
    "        self.activ = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # out,_ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        x = x .transpose(1,2)\n",
    "        \n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.lstm_drop(out)\n",
    "        out = self.linear(out[:, -1, :])  \n",
    "        out = self.linear_drop(out)\n",
    "        out = self.activ(out )\n",
    "        # out = out.transpose(1, 2).reshape(-1, batch_size).transpose(1, 0)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "43eed7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, trainloader, valloader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # model.train().to(device)\n",
    "\n",
    "    val_loss_min = np.inf\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for i_epoch in range(n_epochs):\n",
    "        epoch_train_loss = 0\n",
    "        epoch_train_acc = 0\n",
    "        model.train().to(device)\n",
    "        ## train\n",
    "        for batch in tqdm(trainloader, desc=\"Train\"):\n",
    "            opti.zero_grad()\n",
    "            batch_x = [i[0] for i in batch]\n",
    "            batch_x = torch.stack(batch_x)\n",
    "            batch_y = [i[1] for i in batch]\n",
    "            y_out = model(batch_x)\n",
    "            y_hat = torch.argmax(y_out, dim=1)\n",
    "\n",
    "            ##### Doubt?? do i need to send logits(y_out) or y_hat to criterion ??? \n",
    "            loss = criterion(y_out, torch.tensor(batch_y))\n",
    "            loss.backward()\n",
    "            opti.step()\n",
    "            acc = torch.sum(y_hat==torch.tensor(batch_y))\n",
    "            epoch_train_acc+=acc\n",
    "            epoch_train_loss += loss.item()\n",
    "\n",
    "    \n",
    "        epoch_train_loss = epoch_train_loss/len(trainloader)\n",
    "        epoch_train_acc = epoch_train_acc/len(trainloader)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accuracies.append(epoch_train_acc)\n",
    "\n",
    "        ### val\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_val_loss = 0\n",
    "            epoch_val_acc = 0\n",
    "            for batch in tqdm(valloader, desc=\"Val  \"):\n",
    "                batch_x = [i[0] for i in batch]\n",
    "                batch_x = torch.stack(batch_x)\n",
    "                batch_y = [i[1] for i in batch]\n",
    "                y_out = model(batch_x)\n",
    "                y_hat = torch.argmax(y_out, dim=1)\n",
    "                loss = criterion(y_out, torch.tensor(batch_y))\n",
    "                acc = torch.sum(y_hat==torch.tensor(batch_y))\n",
    "                epoch_val_acc+=acc\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "            epoch_val_loss = epoch_val_loss/len(valloader)\n",
    "            epoch_val_acc = epoch_val_acc/len(valloader)\n",
    "            val_losses.append(epoch_val_loss)\n",
    "            val_accuracies.append(epoch_val_acc)\n",
    "            \n",
    "        # to save model\n",
    "            if epoch_val_loss < val_loss_min:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                tqdm.write(f'Validation loss reduced from {val_loss_min:.6f} to {epoch_val_loss:.6f}, saving model at {save_path} ...')\n",
    "                val_loss_min = epoch_val_loss\n",
    "\n",
    "#         if scheduler:\n",
    "#             scheduler.step()\n",
    "#             tqdm.write(f'Updating lr to {scheduler.get_last_lr()}')\n",
    "\n",
    "        tqdm.write(f'Epoch : {i_epoch+1:02}\\nTrain Loss = {epoch_train_loss:.4f}\\tTrain Acc = {epoch_train_acc}\\n  Val Loss = {epoch_val_loss:.4f}\\t  Val Acc = {epoch_val_acc}\\n')\n",
    "\n",
    "    loss_dict = {\"train_losses\" : train_losses,\n",
    "                 \"val_losses\" : val_losses,\n",
    "                \"train_acc\" : train_accuracies,\n",
    "                \"val_acc\" : val_accuracies}\n",
    "\n",
    "    return loss_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4205873a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size 512\n",
      "LSTMModel(\n",
      "  (layer1): Conv1d(512, 128, kernel_size=(5,), stride=(2,))\n",
      "  (act1): ReLU()\n",
      "  (lstm): LSTM(128, 128, num_layers=4, batch_first=True, bidirectional=True)\n",
      "  (lstm_drop): Dropout(p=0.1, inplace=False)\n",
      "  (linear): Linear(in_features=256, out_features=8, bias=True)\n",
      "  (linear_drop): Dropout(p=0, inplace=False)\n",
      "  (activ): Softmax(dim=1)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|███████████████████████████████████████████████████████████████████████████| 20/20 [02:12<00:00,  6.61s/it]\n",
      "Val  : 100%|█████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss reduced from inf to 2.079650, saving model at model.pt ...\n",
      "Epoch : 01\n",
      "Train Loss = 2.0804\tTrain Acc = 3.700000047683716\n",
      "  Val Loss = 2.0796\t  Val Acc = 3.799999952316284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|███████▌                                                                    | 2/20 [00:23<03:31, 11.77s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m opti \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# opti = optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[89], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(n_epochs, trainloader, valloader)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m##### Doubt?? do i need to send logits(y_out) or y_hat to criterion ??? \u001b[39;00m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_out, torch\u001b[38;5;241m.\u001b[39mtensor(batch_y))\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m opti\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m acc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(y_hat\u001b[38;5;241m==\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(batch_y))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################3\n",
    "it = iter(train_dataloader)\n",
    "first = next(it)\n",
    "input_size=first[0][0].shape[0]\n",
    "# print(f'input_size of {i_epoch}', input_size)\n",
    "print('input_size', input_size)\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "output_size = 8  # Modify this according to your specific task\n",
    "\n",
    "n_epochs = 4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "print(model)\n",
    "opti = optim.Adam(model.parameters())\n",
    "# opti = optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "results = train(n_epochs, train_dataloader, dev_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b86da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc46097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8187656b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
